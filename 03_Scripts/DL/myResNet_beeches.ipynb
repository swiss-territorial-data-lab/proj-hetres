{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transfer Learing as in https://github.com/Shaam93/Building-a-classifer-with-Pytorch/blob/master/Classification_of_flowers_Train_val_test.ipynb and in https://medium.com/p/9574e74d17ad#1650"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import copy\n",
    "\n",
    "import PIL \n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "from torchvision import transforms, models, datasets\n",
    "\n",
    "from torchvision.io import read_image\n",
    "# available ResNet : 18, 34, 50, 101, 152 -layers\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data train-valid-test preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'C:/Users/cmarmy/Documents/STDL/Beeches/DL/data/train'\n",
    "valid_dir = 'C:/Users/cmarmy/Documents/STDL/Beeches/DL/data/valid'\n",
    "test_dir = 'C:/Users/cmarmy/Documents/STDL/Beeches/DL/data/test'\n",
    "\n",
    "dirs = {'train': train_dir,\n",
    "        'valid': valid_dir,\n",
    "        'test' : test_dir\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        #transforms.RandomRotation(45), # between -45 and 45Â°\n",
    "        #transforms.RandomResizedCrop(224), # A crop of random size (default: of 0.08 to 1.0) of the original size and a random aspect ratio (default: of 3/4 to 4/3) \n",
    "        #transforms.RandomHorizontalFlip(),\n",
    "        transforms.Resize(224), \n",
    "        transforms.Pad((0,336), fill=0, padding_mode='constant'),\n",
    "        transforms.Resize(224), \n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(), #numpy.ndarray(H x W x C) in the range [0, 255] to torch.FloatTensor(C x H x W) in the range [0.0, 1.0]\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], # to be applied on tensor only. sequence of means and std for each channel\n",
    "                            [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'valid': transforms.Compose([\n",
    "        transforms.Resize(224), \n",
    "        transforms.Pad((0,336), fill=0, padding_mode='constant'),\n",
    "        transforms.Resize(224), \n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(224), \n",
    "        transforms.Pad((0,336), fill=0, padding_mode='constant'),\n",
    "        transforms.Resize(224), \n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets with ImageFolder\n",
    "# ImageFolder is using the directory structure to create a classes field.\n",
    "# Does not support empty directory\n",
    "image_datasets = {x: datasets.ImageFolder(dirs[x],   transform=data_transforms[x]) for x in ['train', 'valid', 'test']}\n",
    "# load the data into batches\n",
    "# think to devide, drop_last=True if necessary\n",
    "\n",
    "# !! batch size !!\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=1, shuffle=True) for x in ['train', 'valid', 'test']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'valid', 'test']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m class_names \u001b[39m=\u001b[39m image_datasets[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mclasses\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(class_names)\n\u001b[0;32m      4\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mC:/Users/cmarmy/Documents/STDL/Beeches/DL/data/cat_to_name.json\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'image_datasets' is not defined"
     ]
    }
   ],
   "source": [
    "class_names = image_datasets['train'].classes\n",
    "print(class_names)\n",
    "\n",
    "with open('C:/Users/cmarmy/Documents/STDL/Beeches/DL/data/cat_to_name.json', 'r') as f:\n",
    "    label_map = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#check images after transform\n",
    "def imshow(image):\n",
    "    if isinstance(image, torch.Tensor):\n",
    "        image = image.numpy().transpose((1, 2, 0)) # if the image is already a tensor  \n",
    "    else:\n",
    "        image = np.array(image).transpose((1, 2, 0)) # put channel in the right order\n",
    "        \n",
    "    # Unnormalize\n",
    "    mean = np.array([0.485, 0.456, 0.406]) # backward some of the transformations\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    image = std * image + mean\n",
    "    image = np.clip(image, 0, 1)\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(15, 15))\n",
    "    plt.imshow(image)\n",
    "    ax.axis('off') \n",
    "        \n",
    "images, _ = next(iter(dataloaders['train']))\n",
    "out = torchvision.utils.make_grid(images, nrow=8) # plot each batch with 8 element per row \n",
    "imshow(out)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = models.resnet18(pretrained=True) deprecated\n",
    "weights = ResNet18_Weights.DEFAULT\n",
    "model = resnet18(weights=weights)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change in the model (see previous cell) : (fc): Linear(in_features=512, out_features=1000, bias=True) \n",
    "# we have now only 102 classes to predict\n",
    "model.fc = nn.Linear(512, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only CPU\n"
     ]
    }
   ],
   "source": [
    "# Set CPU or GPU\n",
    "# I only have a non-CUDA-compatible GPU card. Could be an alternative https://github.com/artyom-beilis/pytorch_dlprim\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  device= torch.device('cuda')\n",
    "  print(\"CUDA is avaialble\")\n",
    "else:\n",
    "  device= torch.device('cpu')\n",
    "  print(\"Only CPU\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option A : train/validate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\cmarmy\\anaconda3\\envs\\pytorch\\lib\\site-packages\\debugpy\\_vendored\\pydevd\\_pydevd_bundle\\pydevd_vars.py\", line 620, in change_attr_expression\n",
      "    value = eval(expression, frame.f_globals, frame.f_locals)\n",
      "  File \"<string>\", line 1\n",
      "    ResNet(  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  (relu): ReLU(inplace=True)  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)  (layer1): Sequential(    (0): BasicBlock(      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)    )    (1): BasicBlock(      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)    )  )  (layer2): Sequential(    (0): BasicBlock(      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (downsample): Sequential(        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      )    )    (1): BasicBlock(      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)    )  )  (layer3): Sequential(    (0): BasicBlock(      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (downsample): Sequential(        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      )    )    (1): BasicBlock(      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)    )  )  (layer4): Sequential(    (0): BasicBlock(      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (downsample): Sequential(        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      )    )    (1): BasicBlock(      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)    )  )  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))  (fc): Linear(in_features=512, out_features=3, bias=True))\n",
      "                    ^\n",
      "SyntaxError: invalid syntax\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\cmarmy\\anaconda3\\envs\\pytorch\\lib\\site-packages\\debugpy\\_vendored\\pydevd\\_pydevd_bundle\\pydevd_vars.py\", line 620, in change_attr_expression\n",
      "    value = eval(expression, frame.f_globals, frame.f_locals)\n",
      "  File \"<string>\", line 1\n",
      "    ResNet(  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  (relu): ReLU(inplace=True)  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)  (layer1): Sequential(    (0): BasicBlock(      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)    )    (1): BasicBlock(      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)    )  )  (layer2): Sequential(    (0): BasicBlock(      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (downsample): Sequential(        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      )    )    (1): BasicBlock(      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)    )  )  (layer3): Sequential(    (0): BasicBlock(      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (downsample): Sequential(        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      )    )    (1): BasicBlock(      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)    )  )  (layer4): Sequential(    (0): BasicBlock(      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (downsample): Sequential(        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      )    )    (1): BasicBlock(      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (relu): ReLU(inplace=True)      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)    )  )  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))  (fc): Linear(in_features=512, out_features=3, bias=True))\n",
      "                    ^\n",
      "SyntaxError: invalid syntax\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[163], line 82\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[39m# Number of epochs\u001b[39;00m\n\u001b[0;32m     80\u001b[0m eps\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m\n\u001b[1;32m---> 82\u001b[0m model \u001b[39m=\u001b[39m train_model(model, criteria, optimizer, scheduler, eps, \u001b[39m'\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[163], line 40\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, criteria, optimizer, scheduler, num_epochs, device)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mset_grad_enabled(phase \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m     39\u001b[0m     outputs \u001b[39m=\u001b[39m model(inputs) \u001b[39m# inputs, batch of images; outputs, tensor of #image per #classes\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m     _, preds \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(outputs, \u001b[39m1\u001b[39m)\n\u001b[0;32m     41\u001b[0m     loss \u001b[39m=\u001b[39m criteria(outputs, labels) \u001b[39m# labels are the true labels for the batch.\u001b[39;00m\n\u001b[0;32m     43\u001b[0m     \u001b[39m# backward + optimize only if in training phase\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[163], line 40\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, criteria, optimizer, scheduler, num_epochs, device)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mset_grad_enabled(phase \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m     39\u001b[0m     outputs \u001b[39m=\u001b[39m model(inputs) \u001b[39m# inputs, batch of images; outputs, tensor of #image per #classes\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m     _, preds \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(outputs, \u001b[39m1\u001b[39m)\n\u001b[0;32m     41\u001b[0m     loss \u001b[39m=\u001b[39m criteria(outputs, labels) \u001b[39m# labels are the true labels for the batch.\u001b[39;00m\n\u001b[0;32m     43\u001b[0m     \u001b[39m# backward + optimize only if in training phase\u001b[39;00m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1395\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1344\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\cmarmy\\anaconda3\\envs\\pytorch\\lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2067\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[0;32m   2064\u001b[0m             from_this_thread\u001b[39m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[0;32m   2066\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_threads_suspended_single_notification\u001b[39m.\u001b[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001b[1;32m-> 2067\u001b[0m         keep_suspended \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n\u001b[0;32m   2069\u001b[0m frames_list \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   2071\u001b[0m \u001b[39mif\u001b[39;00m keep_suspended:\n\u001b[0;32m   2072\u001b[0m     \u001b[39m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\cmarmy\\anaconda3\\envs\\pytorch\\lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2103\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[0;32m   2100\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_input_hook()\n\u001b[0;32m   2102\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_internal_commands()\n\u001b[1;32m-> 2103\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m0.01\u001b[39;49m)\n\u001b[0;32m   2105\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[39mstr\u001b[39m(\u001b[39mid\u001b[39m(frame)))\n\u001b[0;32m   2107\u001b[0m \u001b[39m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#training/validating function \n",
    "# this part of code is from https://medium.com/@josh_2774/deep-learning-with-pytorch-9574e74d17ad\n",
    "#I changed minor things for my convinience\n",
    "def train_model(model, criteria, optimizer, scheduler,    \n",
    "                                      num_epochs=25, device='cuda'):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict()) # deepcopy() vs. (shallow) copy(): copy the object and the objects inside vs. copy object and reference object inside.\n",
    "    best_acc = 0.0\n",
    "\n",
    "    optimizer.step() # this new line added \n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs ))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]: #iterate on the batches\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs) # inputs, batch of images; outputs, tensor of #image per #classes\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criteria(outputs, labels) # labels are the true labels for the batch.\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward() # Computes the gradient of current tensor w.r.t. graph leaves. \"loss derivatives for the parameters\"\n",
    "                        optimizer.step() # parameter update\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'valid' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "     \n",
    "\n",
    "criteria = nn.CrossEntropyLoss()\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "# Number of epochs\n",
    "eps=5\n",
    "\n",
    "model = train_model(model, criteria, optimizer, scheduler, eps, 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the checkpoint \n",
    "model.class_to_idx = image_datasets['train'].class_to_idx\n",
    "model.cpu()\n",
    "torch.save({\n",
    "            'state_dict': model.state_dict(),\n",
    "             \n",
    "            'class_to_idx': model.class_to_idx}, \n",
    "            \"classifier_beeche.pth\")\n",
    "#torch.save(model.sate_dict, \"classifier.pt\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option B: load trained/validated model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(checkpoint_path):\n",
    "    weights = ResNet18_Weights.DEFAULT\n",
    "    model = resnet18(weights=weights)\n",
    "    model.fc = nn.Linear(512, 3) # !! we changed the model before !!\n",
    "\n",
    "    chpt = torch.load('classifier_beeche.pth') # load trained weights and biases\n",
    "    model.class_to_idx = chpt['class_to_idx']   \n",
    "    model.load_state_dict(chpt['state_dict'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = load_model('classifier.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute accuracy for test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0])\n",
      "tensor([9.7205])\n",
      "tensor([False])\n",
      "tensor(0.)\n",
      "tensor(1.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(1.)\n",
      "tensor(0.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(1.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(0.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(1.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "def calc_accuracy(model, data, cuda=False):\n",
    "    model.eval()\n",
    "    model.to(device='cpu')    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, (inputs, labels) in enumerate(dataloaders[data]):            \n",
    "            if cuda:\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()            \n",
    "                \n",
    "            # obtain the outputs from the model\n",
    "            outputs = model.forward(inputs)     \n",
    "\n",
    "            # max provides the (maximum probability, max value)\n",
    "            _, predicted = outputs.max(dim=1)            \n",
    "            \n",
    "            # check the \n",
    "            if idx == 0:\n",
    "                print(predicted) #the predicted class\n",
    "                print(torch.exp(_)) # the predicted probability, what is exactly the range of probability for ResNet?\n",
    "            equals = predicted == labels.data  \n",
    "\n",
    "            if idx == 0:\n",
    "                print(equals)            \n",
    "            print(equals.float().mean())\n",
    "\n",
    "calc_accuracy(model, 'train', False)\n",
    "\n",
    "# output not right or not understable yet. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate one image on the trained model\n",
    " - resize it as before (224*224), radiometry and channels order "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(image_path):\n",
    "    ''' \n",
    "    Scales, crops, and normalizes a PIL image for a PyTorch       \n",
    "    model, returns an Numpy array\n",
    "    '''\n",
    "    # Open the image\n",
    "    from PIL import Image\n",
    "    img = Image.open(image_path)    \n",
    "    \n",
    "    # Resize\n",
    "    if img.size[0] > img.size[1]:\n",
    "        img.thumbnail((10000, 256))\n",
    "    else:\n",
    "        img.thumbnail((256, 10000))    \n",
    "        \n",
    "    # Crop \n",
    "    # !! adjust size to the current issue !!\n",
    "    left_margin = (img.width-224)/2\n",
    "    bottom_margin = (img.height-112)/2\n",
    "    right_margin = left_margin + 224\n",
    "    top_margin = bottom_margin + 112   \n",
    "     \n",
    "    img = img.crop((left_margin, bottom_margin, right_margin,   \n",
    "                      top_margin))\n",
    "    # Normalize\n",
    "    img = np.array(img)/255\n",
    "    mean = np.array([0.485, 0.456, 0.406]) #provided mean\n",
    "    std = np.array([0.229, 0.224, 0.225]) #provided std\n",
    "    img = (img[:,:,[0,1,2]] - mean)/std\n",
    "    \n",
    "    # Move color channels to first dimension as expected by PyTorch\n",
    "    img = img.transpose((2, 0, 1))\n",
    "    \n",
    "    return img\n",
    "#image_path = r\"C:\\Users\\cmarmy\\Documents\\STDL\\Beeches\\DL\\data\\test\\2\\Malade_189_Profil_1_RASTER_RGB.tif\"\n",
    "#img = process_image(image_path)\n",
    "#imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([100.59130859375], ['1'], ['healthy'])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict(image_path, model, top_num=1):\n",
    "    # Process image\n",
    "    img = process_image(image_path)\n",
    "    \n",
    "    # Numpy -> Tensor\n",
    "    image_tensor = torch.from_numpy(img).type(torch.FloatTensor)    \n",
    "    \n",
    "    # Add batch of size 1 to image\n",
    "    model_input = image_tensor.unsqueeze(0)\n",
    "    \n",
    "    # Probs\n",
    "    probs = torch.exp(model.forward(model_input))\n",
    "    \n",
    "    # Top probs\n",
    "    top_probs, top_labs = probs.topk(top_num)\n",
    "    top_probs = top_probs.detach().numpy().tolist()[0] \n",
    "    top_labs = top_labs.detach().numpy().tolist()[0]\n",
    "    \n",
    "    # Convert indices to classes\n",
    "    idx_to_class = {val: key for key, val in model.class_to_idx.items()}    \n",
    "    top_labels = [idx_to_class[lab] for lab in top_labs]\n",
    "    top_flowers = [label_map[idx_to_class[lab]] for lab in top_labs]\n",
    "    \n",
    "    return top_probs, top_labels, top_flowers\n",
    "\n",
    "image_path = r\"C:\\Users\\cmarmy\\Documents\\STDL\\Beeches\\DL\\data\\train\\2\\Malade_70_Profil_5_RASTER_RGB.tif\"\n",
    "predict(image_path,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([36.250160217285156], ['1'], ['healthy'])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_path = r\"C:\\Users\\cmarmy\\Documents\\STDL\\Beeches\\DL\\data\\train\\2\\Malade_80_Profil_4_RASTER_RGB.tif\"\n",
    "predict(image_path,model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f980756f34b220261659cb6a4d78952f9fcbbc3360208cf724f06c5b4144671d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
